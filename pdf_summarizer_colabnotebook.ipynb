{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS0o6wRVO4Yk"
   },
   "source": [
    "**Task Overview and Problem Definition :**\n",
    " Pdf summarization for the  key elements such as future growth prospects, key changes in the business, key triggers, important information that might have a material effect on next year's earnings and growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the NoteBook change the pdf path in main execution by uploading pdf file to your colab session storage or drive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq6P41ougTm2"
   },
   "source": [
    "**STEPS:**\n",
    "\n",
    "1.Installing Dependencies\n",
    "\n",
    "2.Extract Text from the PDF\n",
    "\n",
    "3.Extracted Text into chunks\n",
    "\n",
    "4.Chunks into Batches\n",
    "\n",
    "5.Summarize Batches Using Groq API and Combine Batch Summaries into Final Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIiXoFy4K5gQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set API key as an environment variable\n",
    "os.environ[\"API_KEY\"] = \"\"\n",
    "\n",
    "# Retrieve the API key when needed\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXW-Tnk6HJZ1"
   },
   "source": [
    "**Step** 1: **Installing** **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_17RjynHRKK",
    "outputId": "da0c6aa4-f844-4fa4-e990-be502c8f7906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.13)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (0.1.137)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (9.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.10.10)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "!pip install langchain\n",
    "!pip install langchain_groq\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5R6_ffTHh47"
   },
   "source": [
    "**Step 2: Text Extraction from pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXC2X4QwHyUh"
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "class PDFExtractor:\n",
    "    def __init__(self, pdf_path):\n",
    "        \"\"\"Initializes the PDFExtractor with the path to the PDF file.\"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.doc = fitz.open(pdf_path)\n",
    "\n",
    "    def extract_text(self):\n",
    "        \"\"\"Extracts all text from the PDF and returns it as a single string.\"\"\"\n",
    "        full_text = \"\"\n",
    "        for page_num in range(len(self.doc)):\n",
    "            page = self.doc[page_num]\n",
    "            full_text += page.get_text(\"text\") + \"\\n\"  # Append text of each page\n",
    "        return full_text\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the document to free up resources.\"\"\"\n",
    "        self.doc.close()\n",
    "\n",
    "# # Usage example\n",
    "# pdf_path = \"/content/drive/MyDrive/SJS Transcript Call.pdf\"  # Replace with your PDF file path\n",
    "# pdf_extractor = PDFExtractor(pdf_path)\n",
    "# pdf_text = pdf_extractor.extract_text()\n",
    "# pdf_extractor.close()\n",
    "# print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtzF0PK9I2kw"
   },
   "source": [
    "**Step 3**: **Extracted Text into chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLPvAHsQI_HP"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "class TextChunker:\n",
    "    def __init__(self, max_words=300):\n",
    "        \"\"\"Initializes the TextChunker with a maximum number of words per chunk.\"\"\"\n",
    "        self.max_words = max_words\n",
    "\n",
    "    def split_text_into_chunks(self, text):\n",
    "        \"\"\"\n",
    "        Splits the given text into chunks with a maximum number of words per chunk.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to split.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of text chunks.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_word_count = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_word_count = len(sentence.split())\n",
    "            if current_word_count + sentence_word_count > self.max_words:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_word_count = 0\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "# text_chunker = TextChunker()\n",
    "# chunked_text = text_chunker.split_text_into_chunks(pdf_text)\n",
    "\n",
    "# print(chunked_text)\n",
    "# # print(type(chunked_text))\n",
    "# # print(len(chunked_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0wE_W_-JISS"
   },
   "source": [
    "**Step 4**: **Chunks into Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqKKahL-Jd7q"
   },
   "outputs": [],
   "source": [
    "class TokenBatcher:\n",
    "    def __init__(self, token_limit=4000, model=\"llama3-8b-8192\"):\n",
    "        self.token_limit = token_limit\n",
    "        self.model = model\n",
    "\n",
    "    def num_tokens_from_string(self, string):\n",
    "        \"\"\"\n",
    "        Calculates the number of tokens in a string.\n",
    "        \"\"\"\n",
    "        return len(string.split())\n",
    "\n",
    "    def create_batches(self, text_chunks):\n",
    "        \"\"\"\n",
    "        Creates batches of text chunks, each within the token limit.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_token_count = 0\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            chunk_tokens = self.num_tokens_from_string(chunk)\n",
    "\n",
    "            if current_token_count + chunk_tokens > self.token_limit:\n",
    "                # If adding this chunk exceeds the token limit, start a new batch\n",
    "                batches.append(current_batch)\n",
    "                current_batch = [chunk]\n",
    "                current_token_count = chunk_tokens\n",
    "            else:\n",
    "                # Add the chunk to the current batch\n",
    "                current_batch.append(chunk)\n",
    "                current_token_count += chunk_tokens\n",
    "\n",
    "        # Add the final batch if there are any remaining chunks\n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "\n",
    "        return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1nRu9C-JkP8"
   },
   "source": [
    "**Step 5: Summarize Batches Using Groq API and Combine Batch Summaries into Final Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwYS30Z9J407"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "class TextSummarizer:\n",
    "    def __init__(self, groq_api_key, model=\"llama3-8b-8192\", temperature=0.7, max_tokens=1000, top_p=1):\n",
    "        \"\"\"\n",
    "        Initializes the TextCleaner class with the Groq API key and default model parameters.\n",
    "\n",
    "        Args:\n",
    "            groq_api_key (str): The API key for accessing the Groq API.\n",
    "            model (str): The Groq model to use for text cleaning.\n",
    "            temperature (float): Controls the randomness in the output.\n",
    "            max_tokens (int): The maximum number of tokens to generate.\n",
    "            top_p (float): Controls the diversity of the output.\n",
    "        \"\"\"\n",
    "        self.client = Groq(api_key=groq_api_key)\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def clean_text(self, chunk):\n",
    "        \"\"\"\n",
    "        Cleans and formats the text chunk to improve readability and focuses on specific investor elements.\n",
    "\n",
    "        Args:\n",
    "            chunk (str): The raw text to be cleaned.\n",
    "            agent_context (str): Optional additional context for the cleaning agent.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned and formatted text.\n",
    "        \"\"\"\n",
    "        # Prepare the cleaning prompt\n",
    "        context = \"\"\"\n",
    "                    You are an AI model trained to analyze financial documents from the perspective of an investor. Your task is to process the provided\n",
    "                    financial data or company report, ensuring that all relevant information is extracted in a way that highlights the most important factors for investors.\n",
    "\n",
    "                    You should clean and process the data by removing any irrelevant sections such as:\n",
    "                    - **Table of contents**, legal disclaimers, boilerplate text, or other generic content that does not provide value to the analysis.\n",
    "                    - **Historical or outdated data** that is not pertinent to forecasting the company's future.\n",
    "                    - **Non-financial content**, like promotional material or unrelated charts and graphs.\n",
    "                    - **Excessive formatting** that may obscure the core financial information.\"\"\"\n",
    "\n",
    "        prompt =\"\"\"Please analyze the following company financial data and provide an investor-focused and make sure to include all numeric values for all metrics\n",
    "\n",
    "                    **Future Growth Prospects**: Identify any signs of potential growth for the company, including new market opportunities, business expansions, or projected increases in revenue. **Ensure that all numeric values** mentioned, such as growth percentages, future revenue figures, or market share projections, are captured and included in your analysis.\n",
    "\n",
    "                    **Key Business Changes**: Highlight significant changes in the company’s structure, operations, or strategy that might affect its future performance. **Include all numeric values** tied to these changes, such as revenue impacts, cost savings, or any figures provided that illustrate how these changes will affect the business.\n",
    "\n",
    "                    **Key Triggers**: Identify any external or internal factors that might significantly affect the company’s financial performance, such as regulatory changes, market trends, or new competitors. **Ensure that all relevant numeric values** are included, such as projected market share shifts, investment figures, or economic impacts linked to these triggers.\n",
    "\n",
    "                    **Impact on Coming Year Earnings and Growth**: Assess how the factors identified in\"\"\" + chunk\n",
    "\n",
    "\n",
    "        # Set up the chat message format for the API call\n",
    "\n",
    "        try:\n",
    "            # Make the completion request to Groq API\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages = [\n",
    "                {\"role\": \"system\", \"content\": context},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "               ],\n",
    "\n",
    "\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "                top_p=self.top_p,\n",
    "                stop=None,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            # Extract the cleaned text\n",
    "            cleaned_text = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "            return cleaned_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Groq API call: {e}\")\n",
    "            return chunk  # Return the original text if there's an error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSUHxOetPzz3"
   },
   "source": [
    "**Main Execution Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EkgqIhZKEu-",
    "outputId": "24ec8e27-9d3d-41a9-b3ef-e8283d805df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary: **Future Growth Prospects:**\n",
      "\n",
      "* The company expects to generate significant growth from the Walter Pack India business, with proforma revenue growth of 48.2% YoY and proforma EBITDA margin expansion of 120 bps.\n",
      "* The company is targeting a growth rate of 20% to 25% organically, driven by the consolidation of SJS, Exotech, and Walter Pack.\n",
      "* Walter Pack is expected to grow at a faster pace than the consolidated entity, driven by its strong demand from customers and new business opportunities.\n",
      "* The company is also expecting growth from its consumer durables segment, driven by its expansion into new product lines and customer relationships.\n",
      "* The company is targeting a growth rate of 50% in FY24, driven by the consolidation of SJS, Exotech, and Walter Pack.\n",
      "* The company is expecting a significant increase in revenue from its exports, driven by its growth in Europe and North America.\n",
      "\n",
      "**Key Business Changes:**\n",
      "\n",
      "* The acquisition of Walter Pack India has added new technologies, customers, manufacturing capabilities, and management bandwidth to the company.\n",
      "* The acquisition has also led to a change in the company's business strategy, with a focus on inorganic growth and cross-selling opportunities between the three businesses.\n",
      "* The company has consolidated its businesses under SJS Enterprises, Exotech, and Walter Pack.\n",
      "* Walter Pack has joined the SJS family, bringing in new technologies and capabilities.\n",
      "* The company has changed its business strategy to focus on higher-margin businesses and reduce its dependence on two-wheelers.\n",
      "\n",
      "**Key Triggers:**\n",
      "\n",
      "* Regulatory changes: The company believes that the acquisition of Walter Pack India will help it to strengthen its market leadership in the aesthetic business.\n",
      "* Market trends: The company is confident that the growth momentum in the automotive and consumer appliances sectors will continue, driven by increasing demand for premium products and technologies.\n",
      "* New competitors: The company believes that its focus on innovation and premium products will help it to stay ahead of new competitors in the market.\n",
      "\n",
      "**Impact on Coming Year Earnings and Growth:**\n",
      "\n",
      "* The company expects to deliver 50% YoY growth in consolidated revenues and 40% YoY growth in consolidated PAT in FY24, driven by the addition of Walter Pack India revenues and robust margin profile in its existing businesses.\n",
      "* The company expects to maintain its guidance of 20% to 25% organic growth for FY24 to FY26, with the addition of Walter Pack India revenues providing further growth momentum.\n",
      "\n",
      "**Numeric Values:**\n",
      "\n",
      "* Revenue growth of 20% to 25% organically.\n",
      "* Revenue growth of 50% in FY24.\n",
      "* EBITDA growth of 40% in FY24.\n",
      "* Exports growing by 11% in the current quarter.\n",
      "* Consumer durables growing by 2% to 3% in the current quarter.\n",
      "* Technical support fees paid by Walter Pack to its parents at 1% of sales.\n",
      "* Revenue of Walter Pack at Rs.356 million in the current quarter.\n",
      "* EBITDA of Walter Pack at Rs.31.5 million in the current quarter.\n",
      "* Order book of 90% confirmed for FY24, excluding WPI.\n",
      "* Order book of 90% confirmed for FY24, including WPI.\n",
      "\n",
      "**Financial Analysis:**\n",
      "\n",
      "* Revenue: The company expects to generate revenue close to Rs.200 crores at full utilization of Walter Pack's capacity.\n",
      "* PAT: The company expects its PAT to grow by 40% over the last year.\n",
      "* CAGR: The company expects its organic growth to be between 20% to 25%.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "* Investors should consider the company's growth prospects and its ability to integrate the technology from Walter Pack.\n",
      "* Investors should also consider the company's financial performance and its ability to maintain its margins in the face of increased competition.\n",
      "* Investors should monitor the company's progress in expanding into new markets and products, as this is expected to drive growth in the long term.\n",
      "\n",
      "**Investment Insights:**\n",
      "\n",
      "* The acquisition of Walter Pack is expected to drive growth and increase the company's revenue.\n",
      "* The company's PAT is expected to grow by 40% over the last year.\n",
      "* The company's organic growth is expected to be between 20% to 25%.\n",
      "* The company's strategy to expand into new markets and products, such as medical devices and television aesthetics, is expected to drive growth in the long term.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# import PDFExtractor\n",
    "# import TextChunker\n",
    "# import TextSummarizer\n",
    "# import TokenBatcher\n",
    "\n",
    "def main():\n",
    "    # Initialize paths and configurations\n",
    "\n",
    "    pdf_path = \"/content/SJS Transcript Call.pdf\"  # Replace with the actual PDF file path\n",
    "     # Replace with your Groq API key\n",
    "\n",
    "\n",
    "\n",
    "    # Step 1: Extract text from the PDF\n",
    "    pdf_extractor = PDFExtractor(pdf_path)\n",
    "    pdf_text = pdf_extractor.extract_text()\n",
    "    pdf_extractor.close()\n",
    "    # print(\"Text extracted from PDF.\")\n",
    "\n",
    "    # Step 2: Split text into chunks\n",
    "    text_chunker = TextChunker(max_words=300)\n",
    "    text_chunks = text_chunker.split_text_into_chunks(pdf_text)\n",
    "    # print(\"Text split into chunks.\")\n",
    "\n",
    "    #step3\n",
    "    token_batcher = TokenBatcher(token_limit=4000)\n",
    "    batches = token_batcher.create_batches(text_chunks)\n",
    "    # print(f\"Created {len(batches)} batches based on token limits.\")\n",
    "\n",
    "    # Step 4: Clean and summarize each batch using Groq API\n",
    "    text_summarizer = TextSummarizer(API_KEY)\n",
    "    final_text=\"\"\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "    # print(f\"Processing Chunk {i+1}...\")\n",
    "        combined_text = \"\\n\".join(batch)\n",
    "        # print(f\"Processing Batch {i + 1}...\")\n",
    "        cleaned_chunk = text_summarizer.clean_text(combined_text)\n",
    "        if cleaned_chunk :\n",
    "            final_text += cleaned_chunk + \"\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    final_summary = text_summarizer.clean_text(final_text)\n",
    "\n",
    "\n",
    "    print(\"Final Summary:\", final_summary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
